#from utils import linear_schedule
path_general:
  dqn7:

dqn_cartpole_50:
    policy: 'MlpPolicy'
    learning_starts : 0
    gamma : 0.995
    learning_rate : 0.0003
    exploration_final_eps : 0.17787752200089602
    exploration_initial_eps : 0.17787752200089602
    exploration_fraction: 1
    target_update_interval : 1000
    buffer_size : 50000
    train_freq : [1, "episode"]
    gradient_steps : -1
    verbose : 0
    batch_size : 1024
    policy_kwargs : 'dict(net_arch=[256, 256])'



sac_cartpole_50:
  policy: 'MlpPolicy'
  learning_rate: !!float 1e-3
  buffer_size: 300000
  batch_size: 1024
  ent_coef: 'auto'
  gamma: 0.99
  tau: 0.02
  train_freq: [1,"episode"] #64
  gradient_steps: -1 #64
  learning_starts: 0 #10000
  use_sde: True
  policy_kwargs: "dict(log_std_init=-0.965524, net_arch=[400, 300])"


#    policy: 'MlpPolicy'
#    gamma: 0.98
#    learning_rate: 0.001292297868387387
#    batch_size: 2048
#    buffer_size: 10000
#    learning_starts: 0
#    train_freq : [1, "episode"]
#    tau: 0.02
#    policy_kwargs: "dict(log_std_init=-0.9655242596737095,net_arch=[400,300],use_sde=True)"


#use_sed: True
sac_cartpole_50test:
    policy: 'MlpPolicy'
    gamma: 0.98
    learning_rate: 0.0045
    batch_size: 2048
    buffer_size: 10000
    learning_starts: 0
    train_freq : 64
  #[1, "episode"]
    tau: 0.02
    policy_kwargs: "dict(log_std_init=-0.7565734113557446,net_arch=[256,256])"

dqn_cartpole_20:
    policy: 'MlpPolicy'
    gamma: 0.95
    learning_rate: 0.0018307754257612685
    batch_size: 512
    buffer_size: 1000000
    exploration_final_eps: 0.02633362231811897
    exploration_fraction: 0.22175283117941602
    target_update_interval: 1
    learning_starts: 10000
    train_freq: [1, "episode"]
    policy_kwargs: "dict(net_arch=[64,128])"

atari:
#    env_wrapper:
#      - stable_baselines3.common.atari_wrappers.AtariWrapper
    frame_stack: 4
    policy: 'CnnPolicy'
    n_timesteps: !!float 1e7
    buffer_size: 10000
    learning_rate: !!float 1e-4
    batch_size: 32
    learning_starts: 100000
    target_update_interval: 1000
    train_freq: 4
    gradient_steps: 1
    exploration_fraction: 0.1
    exploration_final_eps: 0.01
    optimize_memory_usage: True


sac_cartpole_20:
    policy: 'MlpPolicy'
    gamma: 0.99
    learning_rate: 0.005097005125862557
    batch_size: 512
    buffer_size: 100000
    learning_starts: 10000
    train_freq : 16
  #[1, "episode"]
    tau: 0.05
    policy_kwargs: "dict(log_std_init=-0.8553237075917042,net_arch=[400,300],use_sde=True)"